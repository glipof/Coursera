
### Model Development



import pandas as pd 		# see http://pandas.pydata.org

# Read the online file by the URL provides above, and assign it to variable "df"
other_path = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv"
df = pd.read_csv(other_path, header=None)


## Simple Linear Regression (Single Variable, X = Predictor/ Y = Target)

# Import linear_model from scikit-learn and create Regression Object
from sklearn.linear_model import LinearRegression
lm = LinearRegression()

# Define variables
X = df[['highway-mpg']]
Y = df['price']

# Fit the model, i.e. find the parameters b0 and b1
lm.fit(X, Y)

# Obtain prediction
Yhat = lm.predict(X)

# Intercept is b0, Slope/ Coefficitent is b1 (B2+ for Multiple Linear Regression)


## Multiple Linear Regression (Multiple Variables, One Target (Y) & Two+ Predictors (X))

# Extract Predictor Values and Train Model
Z = df[['horsepower'. 'curb-weight', 'engine-size', 'highway']]
lm.fit(Z, df['price'])

# Obtain prediction
Yhat = lm.predict(X)


# Model Evaluation Using Visualization

import seaborn as sns

# Determine if linear (straight line) or non-linear (curvature)
sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0,)

# Use Residual Plot to investigate further
sns.residplot(df['highway-mpg'], df['price'])

# Distribution Plot
ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values", ax=ax1)


## Polynomial Regression (Useful for describing Curvilinear Relationships)

# Can use quadratic (squaring of predictor), cubic (cubing predictor) and higher order (+)

# Calculate polynomial of 3rd order
f = np.polyfit(x, y, 3)
p = np.polyld(f)
print(p)		# Returns model equation


# More Than One Dimension

from sklearn.preprocessing import PolynomialFeatures
pr = PolynomialFeatures(degree = 2, include_bias = False)

ps.fit_transform([1,2], include_bias=False)


# To make things easier, we can Normalize each feature simultaneously

from sklearn.preprocessing import StandardScaler

SCALE = StandardScaler()
SCALE.fit(x_data[['horsepower', 'highway-mpg']])

x_scale = SCALE.transform(x_data[['horsepower'. 'highway-mpg']])


# Use Pipelines to simplify (Normalization -> Polynomial Transform -> Linear Regression)

# First two are transformations, last is prediction

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Create Pipeline Constructor
Input = [('scale',StandardScaler()), ('polynomial', PolynomialFeatures(degree=2)), ('mode', LinearRegression())]
pipe = Pipeline(Input)

# Train pipeline and create prediction
Pipe.fit(df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y)
yhat = Pipe.predict(X[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])


## Measures for In-Sample Evaluation (Means Squared Error(MSE), R-Squared (R^2))

# MSE = Difference between actual and predicted values, then square and mean using scikit

from sklearn.metrics import mean_squared_error
mean_squared_error(df['price'], Yhat)

# R^2 is to determine how close the data is to the fitted regression line

X = df[['highway-mpg']]
Y = df['price']
lm.fit(X, Y)
lm.score(X, y)


## Lab Activities













